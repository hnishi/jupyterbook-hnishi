{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hnishi/handson-language-models/blob/main/fine_tune_jp_bert_part01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia8cA7knqvOV"
   },
   "source": [
    "# huggingface transformers を使って日本語 BERT モデルをファインチューニングして感情分析 (with google colab) part01\n",
    "\n",
    "本記事では、日本語 BERT モデルをファインチューニングして感情分析する方法を解説します。\n",
    "\n",
    "BERT の詳細な解説は、この記事のスコープ外とします。\n",
    "\n",
    "この記事は、part01 です。\n",
    "\n",
    "[part02](https://github.com/hnishi/handson-language-models/blob/main/fine_tune_jp_bert_part02.ipynb) では、まとまったデータセットを使って実際に学習と評価を行っています。\n",
    "\n",
    "すべての記事の目次は以下をご参照ください。\n",
    "\n",
    "https://github.com/hnishi/handson-language-models/blob/main/README.md\n",
    "\n",
    "## 参考\n",
    "\n",
    "- [huggingface transformers ドキュメント](https://huggingface.co/transformers/)\n",
    "- [BERT 論文](https://arxiv.org/abs/1810.04805)\n",
    "- [Fine-tuning a BERT model with transformers](https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5Z_zhsNqvOb"
   },
   "source": [
    "## 必要なライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QtQP6vfqvOb",
    "outputId": "8298537e-61e6-44ee-8b5c-71d113537c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.9MB 5.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 23.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.2MB 25.4MB/s \n",
      "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C9UCrH2nWSoE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv3oF4c_bRyf"
   },
   "source": [
    "## 日本語 BERT の簡単なチュートリアル\n",
    "\n",
    "最初に、huggingface transformers を使った日本語 BERT pre-trained model の使い方や fine tuning の方法を、簡単に見ていくことにします。\n",
    "\n",
    "今回試す事前学習済みモデルとしては、 bert-large-japanese　を利用してみたいと思います。\n",
    "\n",
    "---\n",
    "\n",
    "**補足**\n",
    "\n",
    "最近 (2021-03-05)、東北大学のグループから BERT の日本語 pre-trained models の [version 2](https://github.com/cl-tohoku/bert-japanese/releases/tag/v2.0) が公開されています。\n",
    "\n",
    "このリリースでは、より大きなアーキテクチャの bert-large-japanese モデルも公開されています。\n",
    "\n",
    "- cl-tohoku/bert-base-japanese-v2\n",
    "- cl-tohoku/bert-large-japanese\n",
    "\n",
    "```\n",
    "BERT-base models consist of 12 layers, 768 dimensions of hidden states, and 12 attention heads.\n",
    "BERT-large models consist of 24 layers, 1024 dimensions of hidden states, and 16 attention heads.\n",
    "```\n",
    "\n",
    "- https://huggingface.co/cl-tohoku\n",
    "- https://github.com/cl-tohoku/bert-japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiontpcdjG6G"
   },
   "source": [
    "### Pre-trained Model を使って推論\n",
    "\n",
    "BERT なので、mask された token を予測するように学習されています。\n",
    "\n",
    "したがって、pre-trained model を使って、文章中の穴埋め (文章中の欠損箇所の予測) を行うことができます。\n",
    "\n",
    "以下の２種類のモデルを使って推論してみました。\n",
    "\n",
    "- cl-tohoku/bert-large-japanese\n",
    "- bert-base-multilingual-uncased (BERT の多言語モデル)\n",
    "\n",
    "結果、 `bert-base-multilingual-uncased` のほうが自然な文章となりました。\n",
    "\n",
    "この違いは、学習に使用されたデータセットに依存しているのかと思いましたが、どちらのモデルも wikipedia をデータセットとして利用しているので、謎です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624,
     "referenced_widgets": [
      "6c628f4853784e4484a0ab9ce9076855",
      "c84946aa080f4ce4a48e8a104bdd2dde",
      "9339c51c523e47b6aab12529bb8686f0",
      "d32d4643df554200a1b28ba5bfa6e768",
      "e88eef8bbbf245c39c142d5f908d55fa",
      "ebc43b3d133f421cbcbaa93573303ee9",
      "d1522f0219334b52b2f372ff63d74f9d",
      "9df4797201ae430ea44036d1c47b163b",
      "8aee8bd82ef7455d9808883dbeaf4440",
      "14b3db27c5244f57be3183f9811bc1c9",
      "298235ef8acd44d2a27c5b055a4b055c",
      "2b7eceef24414d33bd9b29ddb6cde17e",
      "98f8799f33cf44c0b63a32781414cf01",
      "9e7eb247a3fe47ef977dfd036f26fcfb",
      "4252b30a5ba145b8a9b7737add6b66b2",
      "dba4a25f9d584a9ebe4520a65684b2bd",
      "d407e117316444a885df0d61d7770e5e",
      "8d4e3f26d3664125b531071b464e7ac7",
      "07ccb0887c804d9c9b052ac81af53be4",
      "e69975f8695a43d78e5936b23ba5cdd5",
      "a6e4bf59be5440b28bd4c608fd032526",
      "a1f56bcbbdf04e958a7ac554bb3a7e10",
      "d98048fe8bfe4d7fb9c252d6c9106dca",
      "18b7478593854b2dbfe1f41d78886d37",
      "4c3c4f83dedc4cecb30538dbd5a7f65e",
      "368bd943f94a4006847cdd73747ea841",
      "c65f826eb78b4f7885b83b2925a0a1d5",
      "bd5c349d0c9844e8a11185282b41aa07",
      "e4a8170180844cccb95d4d9e5696a3f5",
      "eefc667e3be14ef8879a072fc1cc73ab",
      "64fa4ea732594367bf1c7b1bff2f8072",
      "e342186095eb4a4c8517c6d63ae5218e"
     ]
    },
    "id": "BAGc2FbAiawP",
    "outputId": "25e95a40-7361-4daa-836f-e083f4880768"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c628f4853784e4484a0ab9ce9076855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=336.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aee8bd82ef7455d9808883dbeaf4440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1354281605.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d407e117316444a885df0d61d7770e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=236001.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3c4f83dedc4cecb30538dbd5a7f65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=174.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13797800242900848,\n",
       "  'sequence': 'こんにちは 、 私 は お モデルです 。',\n",
       "  'token': 860,\n",
       "  'token_str': 'お'},\n",
       " {'score': 0.09143581241369247,\n",
       "  'sequence': 'こんにちは 、 私 は う モデルです 。',\n",
       "  'token': 856,\n",
       "  'token_str': 'う'},\n",
       " {'score': 0.03621169179677963,\n",
       "  'sequence': 'こんにちは 、 私 は こう モデルです 。',\n",
       "  'token': 11668,\n",
       "  'token_str': 'こう'},\n",
       " {'score': 0.028521882370114326,\n",
       "  'sequence': 'こんにちは 、 私 は 、 モデルです 。',\n",
       "  'token': 828,\n",
       "  'token_str': '、'},\n",
       " {'score': 0.02647402137517929,\n",
       "  'sequence': 'こんにちは 、 私 は いら モデルです 。',\n",
       "  'token': 24523,\n",
       "  'token_str': 'いら'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"cl-tohoku/bert-large-japanese\"\n",
    "\n",
    "unmasker = pipeline('fill-mask', model=model_name)\n",
    "unmasker(\"こんにちは、私は[MASK]モデルです。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624,
     "referenced_widgets": [
      "826c4e22e5074f288f4c3d5ed4ba2bb9",
      "943776ff91ed4aeea9c7b4530a37f212",
      "73825b0e26564f698d6620491863351e",
      "5ed5c444888447cb87405fe7541b24a8",
      "ee9a455144994e0a8e016f2e3748b405",
      "81ee226fb4124fa5abc3058916fe1b24",
      "d181c49dc8134ba1a99bc09d8ac7bb56",
      "52f6827d5ce7420eaa4ef9a4ff3bc54a",
      "fca1cbf93f65400193452846c0277cbd",
      "92ea88bcead941e3a52fd86b6ea4163c",
      "ba2421a9ac5e4fdeaa002176545e4994",
      "cdb23c56ec0b471f952444b6ce2304a2",
      "33f971beb2834b529dd4d077e91cff3f",
      "6a659341cde041f9a23ee74ca4d07cf2",
      "1cb4c96456d243ddac551a9ab4519a5a",
      "145aef17b9ac4f22bdf038a1f808089e",
      "11db93e5146043d095ae232b12ef2f83",
      "0bad75fc35894269afc700af256799dd",
      "c59264dd86c345678ca2550616e92494",
      "1e25a8d6756e41079eb9217757cb58b7",
      "e264f9045cb64bc8946cf5e2a6f8da5a",
      "cd3a4592cf504d0580b9a413bb64504b",
      "950e90c576e44cc69ec2c1a4a1b66e27",
      "42e5f2b4d51d4b53977f567a45ef98ad",
      "63286622467141efb700dd5947ee7ffa",
      "17d627a971c4475b93e2d747d1f8bee1",
      "3589490d24cf42e8a86b23d7e4c8c7c6",
      "2b764f4eeefd4bd6a3a9233e42711c5a",
      "6e0142add595446c8144b11b7af834e0",
      "805f69f4a8644ed0be6189858384d80d",
      "a87592fdfe0b494b86f33921f845f2f3",
      "55c15e5c8f044cc2814669cdce716e01"
     ]
    },
    "id": "WpEtuqseh5H_",
    "outputId": "9bb02bb9-0644-4d91-ba18-36012820bb44"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826c4e22e5074f288f4c3d5ed4ba2bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca1cbf93f65400193452846c0277cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=672271273.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11db93e5146043d095ae232b12ef2f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63286622467141efb700dd5947ee7ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1715180.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.21041248738765717,\n",
       "  'sequence': 'こんにちは 、 私 は 、 モテルてす 。',\n",
       "  'token': 1482,\n",
       "  'token_str': '、'},\n",
       " {'score': 0.12128563225269318,\n",
       "  'sequence': 'こんにちは 、 私 は 元 モテルてす 。',\n",
       "  'token': 2051,\n",
       "  'token_str': '元'},\n",
       " {'score': 0.033908627927303314,\n",
       "  'sequence': 'こんにちは 、 私 は 初 モテルてす 。',\n",
       "  'token': 2178,\n",
       "  'token_str': '初'},\n",
       " {'score': 0.029899440705776215,\n",
       "  'sequence': 'こんにちは 、 私 は 女 モテルてす 。',\n",
       "  'token': 3014,\n",
       "  'token_str': '女'},\n",
       " {'score': 0.021887250244617462,\n",
       "  'sequence': 'こんにちは 、 私 は 男 モテルてす 。',\n",
       "  'token': 5846,\n",
       "  'token_str': '男'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "unmasker = pipeline('fill-mask', model=model_name)\n",
    "unmasker(\"こんにちは、私は[MASK]モデルです。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wZQLyH7l97y"
   },
   "source": [
    "### テキスト分類のための Fine Tuning の手順\n",
    "\n",
    "以下、簡易的に 3 種類のラベル (positive: 2, neutral: 1, negative: 0) のデータを使って fine tuning を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "gzvZ0w_MbRyg",
    "outputId": "941e2e21-812f-4066-e7b7-d06573f88a80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>私はこの映画をみることができて、とても嬉しい。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今日の晩御飯は何だろう。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>猫に足を噛まれて痛い。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      text  label\n",
       "0  私はこの映画をみることができて、とても嬉しい。      2\n",
       "1             今日の晩御飯は何だろう。      1\n",
       "2              猫に足を噛まれて痛い。      0"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確認用のデータセット\n",
    "df = pd.DataFrame([{\"text\": \"私はこの映画をみることができて、とても嬉しい。\", \"label\": 2},\n",
    "                              {\"text\": \"今日の晩御飯は何だろう。\", \"label\": 1},\n",
    "                              {\"text\": \"猫に足を噛まれて痛い。\", \"label\": 0}])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A1a3YPCtbRyi"
   },
   "outputs": [],
   "source": [
    "train_docs = df[\"text\"].tolist()\n",
    "train_labels = df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ur522Vee2VN"
   },
   "source": [
    "### モデルのダウンロード (キャッシュがない場合) と読み込み\n",
    "\n",
    "同時にダウンロードされるトークナイザーを利用して、データセットの text の encoding を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bflrko3NbRyj",
    "outputId": "b55131ab-1247-45b8-b0d1-0c462c76683b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://huggingface.co/transformers/training.html#pytorch\n",
    "\n",
    "model_name = \"cl-tohoku/bert-large-japanese\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B_EAHs-bmSJ3"
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(train_docs, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0QqtCEn1bRyk"
   },
   "outputs": [],
   "source": [
    "# Fine-tuning in native PyTorch\n",
    "\n",
    "# > the AdamW() optimizer which implements gradient bias correction as well as weight decay.\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "labels = torch.tensor(train_labels).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4RT2SwnbRyk"
   },
   "source": [
    "### Fine Tune したモデルで推論\n",
    "\n",
    "学習に使ったデータを入力して推論してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cW2vLoZGbRyl"
   },
   "outputs": [],
   "source": [
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1q--QxrqPf4",
    "outputId": "2700bff8-489b-490f-91d7-dc7ad3bb8da9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.3931503891944885}]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer(\"これは、テストのための文章です\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDUlagMbbRym",
    "outputId": "ead2f3f2-49bb-4763-b0cb-5fd3687af029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はこの映画をみることができて、とても嬉しい。: [{'label': 'LABEL_2', 'score': 0.3935401141643524}]\n",
      "今日の晩御飯は何だろう。: [{'label': 'LABEL_1', 'score': 0.5711930990219116}]\n",
      "猫に足を噛まれて痛い。: [{'label': 'LABEL_0', 'score': 0.5832840204238892}]\n"
     ]
    }
   ],
   "source": [
    "_ = list(map(lambda x: print(f\"{x}: {sentiment_analyzer(x)}\"), train_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNnAs23LXRzx"
   },
   "source": [
    "スコアは低いですが、学習に使ったデータセットに関して、正しいラベルが予測できていることを確認できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj4HfgSIXYHu"
   },
   "source": [
    "## まとめ\n",
    "\n",
    "簡単な文章とラベルを用意して fine tuning する方法を記載しました。\n",
    "\n",
    "[次の記事](https://github.com/hnishi/handson-language-models/blob/main/fine_tune_jp_bert_part02.ipynb) では、より大きなデータセットを使って、より時間のかかる学習を試してみたいと思います。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "fine-tune-jp-bert-part01.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
