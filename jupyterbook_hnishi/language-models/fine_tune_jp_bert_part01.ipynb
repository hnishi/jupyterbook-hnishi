{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hnishi/jupyterbook-hnishi/blob/colab-dev/jupyterbook_hnishi/language-models/fine_tune_jp_bert_part01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia8cA7knqvOV"
   },
   "source": [
    "# huggingface transformers を使って日本語 BERT モデルをファインチューニングして感情分析 (with google colab) part01\n",
    "\n",
    "本記事では、日本語 BERT モデルをファインチューニングして感情分析する方法を解説します。\n",
    "\n",
    "BERT の詳細な解説は、この記事のスコープ外とします。\n",
    "\n",
    "この記事は、part01 です。\n",
    "\n",
    "[part02](https://jupyterbook.hnishi.com/language-models/fine_tune_jp_bert_part02.html) では、まとまったデータセットを使って実際に学習と評価を行っています。\n",
    "\n",
    "## 参考\n",
    "\n",
    "- [huggingface transformers ドキュメント](https://huggingface.co/transformers/)\n",
    "- [BERT 論文](https://arxiv.org/abs/1810.04805)\n",
    "- [Fine-tuning a BERT model with transformers](https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5Z_zhsNqvOb"
   },
   "source": [
    "## 必要なライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QtQP6vfqvOb",
    "outputId": "274e5eaa-dce4-4825-db0f-8b2173eaf4e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 2.1MB 5.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 39.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.2MB 36.4MB/s \n",
      "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C9UCrH2nWSoE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv3oF4c_bRyf"
   },
   "source": [
    "## 日本語 BERT の簡単なチュートリアル\n",
    "\n",
    "最初に、huggingface transformers を使った日本語 BERT pre-trained model の使い方や fine tuning の方法を、簡単に見ていくことにします。\n",
    "\n",
    "今回試す事前学習済みモデルとしては、 bert-large-japanese　を利用してみたいと思います。\n",
    "\n",
    "---\n",
    "\n",
    "**補足**\n",
    "\n",
    "最近 (2021-03-05)、東北大学のグループから BERT の日本語 pre-trained models の [version 2](https://github.com/cl-tohoku/bert-japanese/releases/tag/v2.0) が公開されています。\n",
    "\n",
    "このリリースでは、より大きなアーキテクチャの bert-large-japanese モデルも公開されています。\n",
    "\n",
    "- cl-tohoku/bert-base-japanese-v2\n",
    "- cl-tohoku/bert-large-japanese\n",
    "\n",
    "```\n",
    "BERT-base models consist of 12 layers, 768 dimensions of hidden states, and 12 attention heads.\n",
    "BERT-large models consist of 24 layers, 1024 dimensions of hidden states, and 16 attention heads.\n",
    "```\n",
    "\n",
    "- https://huggingface.co/cl-tohoku\n",
    "- https://github.com/cl-tohoku/bert-japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiontpcdjG6G"
   },
   "source": [
    "### Pre-trained Model を使って推論\n",
    "\n",
    "BERT なので、mask された token を予測するように学習されています。\n",
    "\n",
    "したがって、pre-trained model を使って、文章中の穴埋め (文章中の欠損箇所の予測) を行うことができます。\n",
    "\n",
    "以下の２種類のモデルを使って推論してみました。\n",
    "\n",
    "- cl-tohoku/bert-large-japanese\n",
    "- bert-base-multilingual-uncased (BERT の多言語モデル)\n",
    "\n",
    "結果、 `bert-base-multilingual-uncased` のほうが自然な文章となりました。\n",
    "\n",
    "この違いは、学習に使用されたデータセットに依存しているのかと思いましたが、どちらのモデルも wikipedia をデータセットとして利用しているので、謎です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526,
     "referenced_widgets": [
      "690598622b384c56a60d654aa570015e",
      "65f356c474874dbebe96f6ce302445b5",
      "e05a160f06284c7ba8ec8d0b69e5a5d3",
      "585b1bcc1ea74dee80ebbfa264f4a0fc",
      "43da6d6705194f2486663a9e884000ca",
      "0e66a796976947acb0ae12831c5b46a8",
      "0780ded896a24a3f8cc2af8adb3cd8a4",
      "c58283be3095433582a49aa5dfecd623",
      "9357c15b4abe4737bec49359a2d85d16",
      "6899541265fd408d826eac7d0d76f41f",
      "aaedbc18f99e455e99e5dbf9081205aa",
      "67187e6b636c4b9281a60d2ee91c2f67",
      "96a97eea1b624b579e53952f7c78647e",
      "56034bfb43774f8997375ddbbbfe2d4b",
      "91f6e209474848909ef2ef4573d52e04",
      "9ece47f382184dcc99c1095912c53e57"
     ]
    },
    "id": "BAGc2FbAiawP",
    "outputId": "06feca9e-dbf7-4028-b690-3e85111a7ddf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690598622b384c56a60d654aa570015e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=236001.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9357c15b4abe4737bec49359a2d85d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=174.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13797800242900848,\n",
       "  'sequence': 'こんにちは 、 私 は お モデルです 。',\n",
       "  'token': 860,\n",
       "  'token_str': 'お'},\n",
       " {'score': 0.09143581241369247,\n",
       "  'sequence': 'こんにちは 、 私 は う モデルです 。',\n",
       "  'token': 856,\n",
       "  'token_str': 'う'},\n",
       " {'score': 0.03621169179677963,\n",
       "  'sequence': 'こんにちは 、 私 は こう モデルです 。',\n",
       "  'token': 11668,\n",
       "  'token_str': 'こ う'},\n",
       " {'score': 0.028521882370114326,\n",
       "  'sequence': 'こんにちは 、 私 は 、 モデルです 。',\n",
       "  'token': 828,\n",
       "  'token_str': '、'},\n",
       " {'score': 0.02647402137517929,\n",
       "  'sequence': 'こんにちは 、 私 は いら モデルです 。',\n",
       "  'token': 24523,\n",
       "  'token_str': 'い ら'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"cl-tohoku/bert-large-japanese\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "unmasker = pipeline('fill-mask', model=model_name, tokenizer=tokenizer)\n",
    "unmasker(\"こんにちは、私は[MASK]モデルです。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673,
     "referenced_widgets": [
      "f13a02167d764b988eda9136b562399b",
      "af74c135a26a49dc9082237e31dfd84b",
      "d059b29a6a844271b89dba706a2ed034",
      "55e9e23973e54453811ef4b5c5f5463f",
      "afef59de0f9a43f1b9a839760f8fdda5",
      "ad2adddd6e5f4d7fb79cc7e2112ce2cb",
      "07faee21aeda4948aa3f533a483f8904",
      "c4e2eb29d7f946b6b191d565b2f72a90",
      "bb6fcd3967a94ea3a6fed7eff08fe23c",
      "0b3259af3b8147bd86bda2ad4a2aaabc",
      "ad2a4994b7fa470fb0a8490879b23fbd",
      "5d4ef7c661274c6fbbfd78292b6a89df",
      "91f2dcc4d7bb4d3d99248cfc974970e7",
      "ac9635596c20401fa766cebbbaf116a7",
      "5e4a4dd0adf041cdb178443f7cfebdcd",
      "00396bde13ba44759ce2780aa1305d04",
      "45ddb483066b4f7784fc40e60f94154f",
      "c16e791c92624f79a6ffaa65ecaa0038",
      "d5e235fbddc24591bc6284febc31146d",
      "53c4622a944847dfa1841d14bf0c9f32",
      "e4862ab60d474844b6e5245c72f284ad",
      "d0d48d100f61405a90e267ac68697bc4",
      "b7a9d6466edf4965b79ade41bb198f6c",
      "fea5ef86f3ed471ba5a602a297786459",
      "61f1c2763b8b4240a50ad06d26a4bafa",
      "913e0ffae3074e42bf3163a60cf556c7",
      "a33d41c3dc914876a7424adf0ee5742e",
      "84b2cb5a5733421fa864f98d8679c260",
      "185e986518394f2fa7df0f560a8e5b51",
      "2a1a6064aac64c53af22208f9f30b6ad",
      "a803570adceb48e78891560a85cccd05",
      "6ce8cf535a7a46fd93cc2cc5674f11f6",
      "8ee9e9ede2944ee7bffed5bfb196dd05",
      "5bce01d5b0dd464abd23c9d81eea190f",
      "da4b68244b8f47e7b1b879ef23a8b17f",
      "a426560c0aae4f0db32118bc259857ff",
      "d7ad2a44c3f9435c88822adaf643600b",
      "b5dff8c075e747938ea3b80e3ff88989",
      "c7ea4a1323a04f5784397d65548b7470",
      "611264b953c744caa1fe262f28d15293"
     ]
    },
    "id": "WpEtuqseh5H_",
    "outputId": "2b3adcb0-a736-475e-bb69-57bfb449e6ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13a02167d764b988eda9136b562399b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6fcd3967a94ea3a6fed7eff08fe23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=672271273.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ddb483066b4f7784fc40e60f94154f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f1c2763b8b4240a50ad06d26a4bafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1715180.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee9e9ede2944ee7bffed5bfb196dd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.21041248738765717,\n",
       "  'sequence': 'こんにちは 、 私 は 、 モテルてす 。',\n",
       "  'token': 1482,\n",
       "  'token_str': '、'},\n",
       " {'score': 0.12128563225269318,\n",
       "  'sequence': 'こんにちは 、 私 は 元 モテルてす 。',\n",
       "  'token': 2051,\n",
       "  'token_str': '元'},\n",
       " {'score': 0.033908627927303314,\n",
       "  'sequence': 'こんにちは 、 私 は 初 モテルてす 。',\n",
       "  'token': 2178,\n",
       "  'token_str': '初'},\n",
       " {'score': 0.029899440705776215,\n",
       "  'sequence': 'こんにちは 、 私 は 女 モテルてす 。',\n",
       "  'token': 3014,\n",
       "  'token_str': '女'},\n",
       " {'score': 0.021887250244617462,\n",
       "  'sequence': 'こんにちは 、 私 は 男 モテルてす 。',\n",
       "  'token': 5846,\n",
       "  'token_str': '男'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "unmasker = pipeline('fill-mask', model=model_name)\n",
    "unmasker(\"こんにちは、私は[MASK]モデルです。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wZQLyH7l97y"
   },
   "source": [
    "## テキスト分類のための Fine Tuning の手順\n",
    "\n",
    "\n",
    "以下、簡易的に 3 種類のラベル (positive: 2, neutral: 1, negative: 0) のデータを使って fine tuning を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "gzvZ0w_MbRyg",
    "outputId": "a1bb1a5e-77e5-4171-d169-fbf38431ced0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>私はこの映画をみることができて、とても嬉しい。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今日の晩御飯は何だろう。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>猫に足を噛まれて痛い。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      text  label\n",
       "0  私はこの映画をみることができて、とても嬉しい。      2\n",
       "1             今日の晩御飯は何だろう。      1\n",
       "2              猫に足を噛まれて痛い。      0"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確認用のデータセット\n",
    "df = pd.DataFrame([{\"text\": \"私はこの映画をみることができて、とても嬉しい。\", \"label\": 2},\n",
    "                              {\"text\": \"今日の晩御飯は何だろう。\", \"label\": 1},\n",
    "                              {\"text\": \"猫に足を噛まれて痛い。\", \"label\": 0}])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A1a3YPCtbRyi"
   },
   "outputs": [],
   "source": [
    "train_docs = df[\"text\"].tolist()\n",
    "train_labels = df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ur522Vee2VN"
   },
   "source": [
    "## 学習\n",
    "\n",
    "同時にダウンロードされるトークナイザーを利用して、データセットの text の encoding を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bflrko3NbRyj"
   },
   "outputs": [],
   "source": [
    "# Ref: https://huggingface.co/transformers/training.html#pytorch\n",
    "\n",
    "model_name = \"cl-tohoku/bert-large-japanese\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_EAHs-bmSJ3"
   },
   "outputs": [],
   "source": [
    "encodings = tokenizer(train_docs, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0QqtCEn1bRyk"
   },
   "outputs": [],
   "source": [
    "# Fine-tuning in native PyTorch\n",
    "\n",
    "# > the AdamW() optimizer which implements gradient bias correction as well as weight decay.\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "labels = torch.tensor(train_labels).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4RT2SwnbRyk"
   },
   "source": [
    "## Fine Tune したモデルで推論\n",
    "\n",
    "学習に使ったデータを入力して推論してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cW2vLoZGbRyl"
   },
   "outputs": [],
   "source": [
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1q--QxrqPf4",
    "outputId": "c625415e-9fee-4015-f53d-dab4da8e14db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.4746145009994507}]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer(\"これは、テストのための文章です\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDUlagMbbRym",
    "outputId": "a698f83c-b58f-4ce2-9d80-9e21ee55de81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はこの映画をみることができて、とても嬉しい。: [{'label': 'LABEL_2', 'score': 0.49736475944519043}]\n",
      "今日の晩御飯は何だろう。: [{'label': 'LABEL_1', 'score': 0.3745356500148773}]\n",
      "猫に足を噛まれて痛い。: [{'label': 'LABEL_0', 'score': 0.5765560269355774}]\n"
     ]
    }
   ],
   "source": [
    "_ = list(map(lambda x: print(f\"{x}: {sentiment_analyzer(x)}\"), train_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNnAs23LXRzx"
   },
   "source": [
    "スコアは低いですが、学習に使ったデータセットに関して、正しいラベルが予測できていることを確認できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj4HfgSIXYHu"
   },
   "source": [
    "## まとめ\n",
    "\n",
    "簡単な文章とラベルを用意して fine tuning する方法を記載しました。\n",
    "\n",
    "[次の記事](https://jupyterbook.hnishi.com/language-models/fine_tune_jp_bert_part02.html) では、より大きなデータセットを使って、より時間のかかる学習を試してみたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWtJz3m11QwS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "fine-tune-jp-bert-part01.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
