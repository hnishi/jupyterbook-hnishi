{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hnishi/jupyterbook-hnishi/blob/update_bert_fine_tuning/jupyterbook_hnishi/language-models/fine_tune_jp_bert_part01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia8cA7knqvOV"
      },
      "source": [
        "# Hugging Face transformers を使って日本語 BERT モデルをファインチューニングして感情分析 (with google colab) part01\n",
        "\n",
        "本記事では、日本語 BERT モデルをファインチューニングして感情分析する方法を解説します。\n",
        "\n",
        "BERT の詳細な解説は、この記事のスコープ外とします。\n",
        "\n",
        "この記事は、part01 です。\n",
        "\n",
        "[part02](https://jupyterbook.hnishi.com/language-models/fine_tune_jp_bert_part02.html) では、まとまったデータセットを使って実際に学習と評価を行っています。\n",
        "\n",
        "## 補足\n",
        "\n",
        "**Hugging Face とは**\n",
        "\n",
        "Hugging Faceは、学習済みモデルやデータセットなどを公開するリポジトリサイトであると同時に、これらを手軽に利用するためのインタフェースともいえるようなライブラリを公開しています。\n",
        "\n",
        "**BERT**\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805) とは、Bidirectional Encoder Representations from Transformersの略称で、Googleが開発した自然言語処理のモデルです。\n",
        "\n",
        "ラベルのないテキストから文章の中の単語やフレーズの意味や関係性を事前学習し、出力層を1つ追加してファインチューニングを行うことで、幅広いタスクに対して性能の良いモデルを作成できます。\n",
        "\n",
        "## 参考\n",
        "\n",
        "- [huggingface transformers ドキュメント](https://huggingface.co/transformers/)\n",
        "- [BERT 論文](https://arxiv.org/abs/1810.04805)\n",
        "- [Fine-tuning a BERT model with transformers](https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5Z_zhsNqvOb"
      },
      "source": [
        "## 必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QtQP6vfqvOb"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers fugashi[unidic-lite]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9UCrH2nWSoE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, pipeline\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv3oF4c_bRyf"
      },
      "source": [
        "## 日本語 BERT の簡単なチュートリアル\n",
        "\n",
        "最初に、huggingface transformers を使った日本語 BERT pre-trained model の使い方や fine tuning の方法を、見ていきます。\n",
        "\n",
        "今回試す事前学習済みモデルとして、東北大学のグループによって公開されているものを利用します。\n",
        "\n",
        "**参考**\n",
        "\n",
        "- https://huggingface.co/cl-tohoku\n",
        "- https://github.com/cl-tohoku/bert-japanese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiontpcdjG6G"
      },
      "source": [
        "### Pre-trained Model を使って推論\n",
        "\n",
        "BERT モデルは、mask された token (`[MASK]`) を予測するように学習されています。\n",
        "\n",
        "したがって、pre-trained model を使って、文章中の穴埋め (文章中の欠損箇所の予測) を\n",
        "行えます。\n",
        "\n",
        "以下の２種類のモデルを使って推論を試して、結果を比較してみましょう。\n",
        "\n",
        "- [cl-tohoku/bert-large-japanese](https://huggingface.co/cl-tohoku/bert-large-japanese)\n",
        "- [bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased) (BERT の多言語モデル)\n",
        "\n",
        "[pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) に `fill-mask` タスクを指定することで、簡単に試すことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAGc2FbAiawP"
      },
      "outputs": [],
      "source": [
        "model_name = \"cl-tohoku/bert-large-japanese\"\n",
        "\n",
        "unmasker = pipeline('fill-mask', model=model_name)\n",
        "unmasker(\"今日の昼食は[MASK]でした。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpEtuqseh5H_"
      },
      "outputs": [],
      "source": [
        "model_name = \"bert-base-multilingual-uncased\"\n",
        "\n",
        "unmasker = pipeline('fill-mask', model=model_name)\n",
        "unmasker(\"今日の昼食は[MASK]でした。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "\n",
        "`unmasker` に別の文章を渡して推論させてみましょう。"
      ],
      "metadata": {
        "id": "J2snK4rM7b6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning 前の感情分析\n",
        "\n",
        "感情分析タスクは [\"sentiment-analysis\"](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextClassificationPipeline) を `pipeline` のタスクに指定することで行えます。\n",
        "\n",
        "pipeline で利用可能なタスク一覧は、[Hugging Face のドキュメント](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/pipelines) で確認できます。\n",
        "\n",
        "ただし、モデルが指定されたタスクに対応している必要があります。"
      ],
      "metadata": {
        "id": "dKsrZJem9LST"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wZQLyH7l97y"
      },
      "source": [
        "## テキスト分類のための Fine Tuning の手順\n",
        "\n",
        "本来であれば、もっと大規模な学習データセットを用意するべきですが、ここでは説明の簡素化のために、単純なサンプルを手作業で作成して手順を確認します。\n",
        "\n",
        "以下のように、3 種類のラベル (positive: 2, neutral: 1, negative: 0) のデータを用意します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzvZ0w_MbRyg"
      },
      "outputs": [],
      "source": [
        "# 確認用のデータセット\n",
        "df = pd.DataFrame(\n",
        "    [\n",
        "        {\"text\": \"私はこの映画をみることができて、とても嬉しい。\", \"label\": \"POSITIVE\"},\n",
        "        {\"text\": \"今日の晩御飯は何だろう。\", \"label\": \"NEUTRAL\"},\n",
        "        {\"text\": \"猫に足を噛まれて痛い。\", \"label\": \"NEGATIVE\"}\n",
        "     ]\n",
        ")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1a3YPCtbRyi"
      },
      "outputs": [],
      "source": [
        "train_docs = df[\"text\"].tolist()\n",
        "train_labels = df[\"label\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ur522Vee2VN"
      },
      "source": [
        "## 学習\n",
        "\n",
        "同時にダウンロードされるトークナイザーを利用して、データセットの text の encoding を行います。\n",
        "\n",
        "**参考**\n",
        "\n",
        "- https://huggingface.co/transformers/training.html#pytorch\n",
        "- https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
        "- https://huggingface.co/transformers/v4.4.2/custom_datasets.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bflrko3NbRyj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"cl-tohoku/bert-large-japanese\"\n",
        "\n",
        "id2label = {0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\",}\n",
        "label2id = {\"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, id2label=id2label, label2id=label2id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_EAHs-bmSJ3"
      },
      "outputs": [],
      "source": [
        "encodings = tokenizer(train_docs, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "input_ids = encodings['input_ids']\n",
        "attention_mask = encodings['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QqtCEn1bRyk"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning in native PyTorch\n",
        "\n",
        "# the AdamW() optimizer which implements gradient bias correction as well as weight decay.\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "labels = [label2id[label] for label in train_labels]\n",
        "labels = torch.tensor(labels).unsqueeze(0)\n",
        "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "loss = outputs.loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4RT2SwnbRyk"
      },
      "source": [
        "## Fine Tune したモデルで推論\n",
        "\n",
        "学習データ量が少ないので性能に期待できませんが、推論の手順を確認します。\n",
        "\n",
        "以下のように、推論できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW2vLoZGbRyl"
      },
      "outputs": [],
      "source": [
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1q--QxrqPf4"
      },
      "outputs": [],
      "source": [
        "sentiment_analyzer(\"これは、テストのための文章です\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDUlagMbbRym"
      },
      "outputs": [],
      "source": [
        "# 学習データに対する推論\n",
        "_ = list(map(lambda x: print(f\"{x}: {sentiment_analyzer(x)}\"), train_docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題**\n",
        "\n",
        "`sentiment_analyzer` に任意の文章を渡して推論してみましょう。\n",
        "\n"
      ],
      "metadata": {
        "id": "XF_EgNMgbiEj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj4HfgSIXYHu"
      },
      "source": [
        "## まとめ\n",
        "\n",
        "簡単な文章とラベルを用意して fine tuning する方法を記載しました。\n",
        "\n",
        "[次の記事](https://jupyterbook.hnishi.com/language-models/fine_tune_jp_bert_part02.html) では、より大きなデータセットを使って、より時間のかかる学習を試してみたいと思います。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "fine-tune-jp-bert-part01.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}