<a href="https://colab.research.google.com/github/hnishi/handson-language-models/blob/main/fine_tune_jp_bert_part01.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# huggingface transformers ã‚’ä½¿ã£ã¦æ—¥æœ¬èª BERT ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ„Ÿæƒ…åˆ†æ (with google colab) part01

æœ¬è¨˜äº‹ã§ã¯ã€æ—¥æœ¬èª BERT ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦æ„Ÿæƒ…åˆ†æã™ã‚‹æ–¹æ³•ã‚’è§£èª¬ã—ã¾ã™ã€‚

BERT ã®è©³ç´°ãªè§£èª¬ã¯ã€ã“ã®è¨˜äº‹ã®ã‚¹ã‚³ãƒ¼ãƒ—å¤–ã¨ã—ã¾ã™ã€‚

ã“ã®è¨˜äº‹ã¯ã€part01 ã§ã™ã€‚

[part02](https://github.com/hnishi/handson-language-models/blob/main/fine_tune_jp_bert_part02.ipynb) ã§ã¯ã€ã¾ã¨ã¾ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦å®Ÿéš›ã«å­¦ç¿’ã¨è©•ä¾¡ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚

ã™ã¹ã¦ã®è¨˜äº‹ã®ç›®æ¬¡ã¯ä»¥ä¸‹ã‚’ã”å‚ç…§ãã ã•ã„ã€‚

https://github.com/hnishi/handson-language-models/blob/main/README.md

## å‚è€ƒ

- [huggingface transformers ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/transformers/)
- [BERT è«–æ–‡](https://arxiv.org/abs/1810.04805)
- [Fine-tuning a BERT model with transformers](https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b)

## å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«


```python
!pip install -q transformers
```

    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.9MB 5.4MB/s 
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 23.9MB/s 
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 25.4MB/s 
    [?25h  Building wheel for sacremoses (setup.py) ... [?25l[?25hdone



```python
import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, pipeline
from transformers import AdamW
```

## æ—¥æœ¬èª BERT ã®ç°¡å˜ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

æœ€åˆã«ã€huggingface transformers ã‚’ä½¿ã£ãŸæ—¥æœ¬èª BERT pre-trained model ã®ä½¿ã„æ–¹ã‚„ fine tuning ã®æ–¹æ³•ã‚’ã€ç°¡å˜ã«è¦‹ã¦ã„ãã“ã¨ã«ã—ã¾ã™ã€‚

ä»Šå›è©¦ã™äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯ã€ bert-large-japaneseã€€ã‚’åˆ©ç”¨ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚

---

**è£œè¶³**

æœ€è¿‘ (2021-03-05)ã€æ±åŒ—å¤§å­¦ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰ BERT ã®æ—¥æœ¬èª pre-trained models ã® [version 2](https://github.com/cl-tohoku/bert-japanese/releases/tag/v2.0) ãŒå…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

ã“ã®ãƒªãƒªãƒ¼ã‚¹ã§ã¯ã€ã‚ˆã‚Šå¤§ããªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã® bert-large-japanese ãƒ¢ãƒ‡ãƒ«ã‚‚å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

- cl-tohoku/bert-base-japanese-v2
- cl-tohoku/bert-large-japanese

```
BERT-base models consist of 12 layers, 768 dimensions of hidden states, and 12 attention heads.
BERT-large models consist of 24 layers, 1024 dimensions of hidden states, and 16 attention heads.
```

- https://huggingface.co/cl-tohoku
- https://github.com/cl-tohoku/bert-japanese

### Pre-trained Model ã‚’ä½¿ã£ã¦æ¨è«–

BERT ãªã®ã§ã€mask ã•ã‚ŒãŸ token ã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã‚Œã¦ã„ã¾ã™ã€‚

ã—ãŸãŒã£ã¦ã€pre-trained model ã‚’ä½¿ã£ã¦ã€æ–‡ç« ä¸­ã®ç©´åŸ‹ã‚ (æ–‡ç« ä¸­ã®æ¬ æç®‡æ‰€ã®äºˆæ¸¬) ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

ä»¥ä¸‹ã®ï¼’ç¨®é¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦æ¨è«–ã—ã¦ã¿ã¾ã—ãŸã€‚

- cl-tohoku/bert-large-japanese
- bert-base-multilingual-uncased (BERT ã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«)

çµæœã€ `bert-base-multilingual-uncased` ã®ã»ã†ãŒè‡ªç„¶ãªæ–‡ç« ã¨ãªã‚Šã¾ã—ãŸã€‚

ã“ã®é•ã„ã¯ã€å­¦ç¿’ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ä¾å­˜ã—ã¦ã„ã‚‹ã®ã‹ã¨æ€ã„ã¾ã—ãŸãŒã€ã©ã¡ã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚‚ wikipedia ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦åˆ©ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€è¬ã§ã™ã€‚


```python
model_name = "cl-tohoku/bert-large-japanese"

unmasker = pipeline('fill-mask', model=model_name)
unmasker("ã“ã‚“ã«ã¡ã¯ã€ç§ã¯[MASK]ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚")
```


    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=336.0, style=ProgressStyle(description_â€¦


    



    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1354281605.0, style=ProgressStyle(descrâ€¦


    



    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=236001.0, style=ProgressStyle(descriptiâ€¦


    



    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=174.0, style=ProgressStyle(description_â€¦


    


    Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
    - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).





    [{'score': 0.13797800242900848,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ ãŠ ãƒ¢ãƒ‡ãƒ«ã§ã™ ã€‚',
      'token': 860,
      'token_str': 'ãŠ'},
     {'score': 0.09143581241369247,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ ã† ãƒ¢ãƒ‡ãƒ«ã§ã™ ã€‚',
      'token': 856,
      'token_str': 'ã†'},
     {'score': 0.03621169179677963,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ ã“ã† ãƒ¢ãƒ‡ãƒ«ã§ã™ ã€‚',
      'token': 11668,
      'token_str': 'ã“ã†'},
     {'score': 0.028521882370114326,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ ã€ ãƒ¢ãƒ‡ãƒ«ã§ã™ ã€‚',
      'token': 828,
      'token_str': 'ã€'},
     {'score': 0.02647402137517929,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ ã„ã‚‰ ãƒ¢ãƒ‡ãƒ«ã§ã™ ã€‚',
      'token': 24523,
      'token_str': 'ã„ã‚‰'}]




```python
model_name = "bert-base-multilingual-uncased"

unmasker = pipeline('fill-mask', model=model_name)
unmasker("ã“ã‚“ã«ã¡ã¯ã€ç§ã¯[MASK]ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚")
```


    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_â€¦


    



    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=672271273.0, style=ProgressStyle(descriâ€¦


    



    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descriptiâ€¦


    



    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1715180.0, style=ProgressStyle(descriptâ€¦


    


    Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
    - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).





    [{'score': 0.21041248738765717,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ ã€ ãƒ¢ãƒ†ãƒ«ã¦ã™ ã€‚',
      'token': 1482,
      'token_str': 'ã€'},
     {'score': 0.12128563225269318,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ å…ƒ ãƒ¢ãƒ†ãƒ«ã¦ã™ ã€‚',
      'token': 2051,
      'token_str': 'å…ƒ'},
     {'score': 0.033908627927303314,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ åˆ ãƒ¢ãƒ†ãƒ«ã¦ã™ ã€‚',
      'token': 2178,
      'token_str': 'åˆ'},
     {'score': 0.029899440705776215,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ å¥³ ãƒ¢ãƒ†ãƒ«ã¦ã™ ã€‚',
      'token': 3014,
      'token_str': 'å¥³'},
     {'score': 0.021887250244617462,
      'sequence': 'ã“ã‚“ã«ã¡ã¯ ã€ ç§ ã¯ ç”· ãƒ¢ãƒ†ãƒ«ã¦ã™ ã€‚',
      'token': 5846,
      'token_str': 'ç”·'}]



### ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®ãŸã‚ã® Fine Tuning ã®æ‰‹é †

ä»¥ä¸‹ã€ç°¡æ˜“çš„ã« 3 ç¨®é¡ã®ãƒ©ãƒ™ãƒ« (positive: 2, neutral: 1, negative: 0) ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ fine tuning ã‚’è¡Œã„ã¾ã™ã€‚


```python
# ç¢ºèªç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
df = pd.DataFrame([{"text": "ç§ã¯ã“ã®æ˜ ç”»ã‚’ã¿ã‚‹ã“ã¨ãŒã§ãã¦ã€ã¨ã¦ã‚‚å¬‰ã—ã„ã€‚", "label": 2},
                              {"text": "ä»Šæ—¥ã®æ™©å¾¡é£¯ã¯ä½•ã ã‚ã†ã€‚", "label": 1},
                              {"text": "çŒ«ã«è¶³ã‚’å™›ã¾ã‚Œã¦ç—›ã„ã€‚", "label": 0}])
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ç§ã¯ã“ã®æ˜ ç”»ã‚’ã¿ã‚‹ã“ã¨ãŒã§ãã¦ã€ã¨ã¦ã‚‚å¬‰ã—ã„ã€‚</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ä»Šæ—¥ã®æ™©å¾¡é£¯ã¯ä½•ã ã‚ã†ã€‚</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>çŒ«ã«è¶³ã‚’å™›ã¾ã‚Œã¦ç—›ã„ã€‚</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
train_docs = df["text"].tolist()
train_labels = df["label"].tolist()
```

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆ) ã¨èª­ã¿è¾¼ã¿

åŒæ™‚ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’åˆ©ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® text ã® encoding ã‚’è¡Œã„ã¾ã™ã€‚


```python
# Ref: https://huggingface.co/transformers/training.html#pytorch

model_name = "cl-tohoku/bert-large-japanese"

model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)
tokenizer = BertTokenizer.from_pretrained(model_name)
```

    Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese and are newly initialized: ['classifier.weight', 'classifier.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



```python
encodings = tokenizer(train_docs, return_tensors='pt', padding=True, truncation=True, max_length=128)
input_ids = encodings['input_ids']
attention_mask = encodings['attention_mask']
```


```python
# Fine-tuning in native PyTorch

# > the AdamW() optimizer which implements gradient bias correction as well as weight decay.
optimizer = AdamW(model.parameters(), lr=1e-5)

labels = torch.tensor(train_labels).unsqueeze(0)
outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
loss = outputs.loss
loss.backward()
optimizer.step()
```

### Fine Tune ã—ãŸãƒ¢ãƒ‡ãƒ«ã§æ¨è«–

å­¦ç¿’ã«ä½¿ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ã¦æ¨è«–ã—ã¦ã¿ã¾ã™ã€‚


```python
sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=model_name)
```


```python
sentiment_analyzer("ã“ã‚Œã¯ã€ãƒ†ã‚¹ãƒˆã®ãŸã‚ã®æ–‡ç« ã§ã™")
```




    [{'label': 'LABEL_0', 'score': 0.3931503891944885}]




```python
_ = list(map(lambda x: print(f"{x}: {sentiment_analyzer(x)}"), train_docs))
```

    ç§ã¯ã“ã®æ˜ ç”»ã‚’ã¿ã‚‹ã“ã¨ãŒã§ãã¦ã€ã¨ã¦ã‚‚å¬‰ã—ã„ã€‚: [{'label': 'LABEL_2', 'score': 0.3935401141643524}]
    ä»Šæ—¥ã®æ™©å¾¡é£¯ã¯ä½•ã ã‚ã†ã€‚: [{'label': 'LABEL_1', 'score': 0.5711930990219116}]
    çŒ«ã«è¶³ã‚’å™›ã¾ã‚Œã¦ç—›ã„ã€‚: [{'label': 'LABEL_0', 'score': 0.5832840204238892}]


ã‚¹ã‚³ã‚¢ã¯ä½ã„ã§ã™ãŒã€å­¦ç¿’ã«ä½¿ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é–¢ã—ã¦ã€æ­£ã—ã„ãƒ©ãƒ™ãƒ«ãŒäºˆæ¸¬ã§ãã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã§ãã¾ã—ãŸã€‚

## ã¾ã¨ã‚

ç°¡å˜ãªæ–‡ç« ã¨ãƒ©ãƒ™ãƒ«ã‚’ç”¨æ„ã—ã¦ fine tuning ã™ã‚‹æ–¹æ³•ã‚’è¨˜è¼‰ã—ã¾ã—ãŸã€‚

[æ¬¡ã®è¨˜äº‹](https://github.com/hnishi/handson-language-models/blob/main/fine_tune_jp_bert_part02.ipynb) ã§ã¯ã€ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€ã‚ˆã‚Šæ™‚é–“ã®ã‹ã‹ã‚‹å­¦ç¿’ã‚’è©¦ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚
